=> fetching img pairs in /data2/datasets/ruoguli/PS_Blobby_Dataset
	 Found Data: 25661 Train and 259 Val
	 Train Batch 64, Val Batch: 8
Creating Model PS_FCN_CBN
[Network Input] Color image as input
[Network Input] Adding Light direction as input
[Network Input] Input channel: 6
Conv pad = 1
Conv pad = 1
Conv pad = 1
=> using pre-trained model /data2/datasets/ruoguli/idl_project_datas/PS-FCN_B_S_32.pth.tar
Detailed loading log saved to: logs/loading_log_20250416-032255.txt
PS_FCN_CBN(
  (brdf_embed_block): BRDF_EmbedBlock(
    (embed_transform): Linear(in_features=14, out_features=512, bias=True)
  )
  (extractor): FeatExtractor_CBN(
    (conv1): Conditional_Conv(
      (regular_conv): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cBatchNorm): CBN(
        (fc_gamma): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=64, bias=True)
        )
        (fc_beta): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=64, bias=True)
        )
      )
      (activation): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv2): Conditional_Conv(
      (regular_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (cBatchNorm): CBN(
        (fc_gamma): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=128, bias=True)
        )
        (fc_beta): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (activation): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv3): Conditional_Conv(
      (regular_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cBatchNorm): CBN(
        (fc_gamma): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=128, bias=True)
        )
        (fc_beta): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (activation): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv4): Conditional_Conv(
      (regular_conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (cBatchNorm): CBN(
        (fc_gamma): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=256, bias=True)
        )
        (fc_beta): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (activation): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv5): Conditional_Conv(
      (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cBatchNorm): CBN(
        (fc_gamma): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=256, bias=True)
        )
        (fc_beta): Sequential(
          (0): Linear(in_features=512, out_features=512, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (activation): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv6): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv7): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.1, inplace=True)
    )
  )
  (regressor): Regressor_CBN(
    (deconv1): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv2): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv3): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (est_normal): Sequential(
      (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
=> Model Parameters: 5699584
=> Using adam solver for optimization
=> Using cos for criterion normal
---- Start Training Epoch 1: 401 batches ----
---- Start val Epoch 1: 33 batches ----
Best model saved at epoch 1
Save epoch model at 1
Epoch 1: train_loss 0.3011, val_n_err 8.0794
---- Start Training Epoch 2: 401 batches ----
---- Start val Epoch 2: 33 batches ----
Best model saved at epoch 2
Save epoch model at 2
Epoch 2: train_loss 0.2788, val_n_err 6.6498
---- Start Training Epoch 3: 401 batches ----
---- Start val Epoch 3: 33 batches ----
Save epoch model at 3
Epoch 3: train_loss 0.2796, val_n_err 7.1168
---- Start Training Epoch 4: 401 batches ----
---- Start val Epoch 4: 33 batches ----
Save epoch model at 4
Epoch 4: train_loss 0.2780, val_n_err 7.7539
---- Start Training Epoch 5: 401 batches ----
---- Start val Epoch 5: 33 batches ----
Save epoch model at 5
Epoch 5: train_loss 0.2767, val_n_err 7.0647
---- Start Training Epoch 6: 401 batches ----
---- Start val Epoch 6: 33 batches ----
Best model saved at epoch 6
Save epoch model at 6
Epoch 6: train_loss 0.2747, val_n_err 6.3223
---- Start Training Epoch 7: 401 batches ----
---- Start val Epoch 7: 33 batches ----
Best model saved at epoch 7
Save epoch model at 7
Epoch 7: train_loss 0.2724, val_n_err 6.1609
---- Start Training Epoch 8: 401 batches ----
---- Start val Epoch 8: 33 batches ----
Best model saved at epoch 8
Save epoch model at 8
Epoch 8: train_loss 0.2770, val_n_err 5.6002
---- Start Training Epoch 9: 401 batches ----
---- Start val Epoch 9: 33 batches ----
Save epoch model at 9
Epoch 9: train_loss 0.2769, val_n_err 6.1645
---- Start Training Epoch 10: 401 batches ----
---- Start val Epoch 10: 33 batches ----
Save epoch model at 10
Epoch 10: train_loss 0.2753, val_n_err 6.4809
---- Start Training Epoch 11: 401 batches ----
---- Start val Epoch 11: 33 batches ----
Save epoch model at 11
Epoch 11: train_loss 0.2773, val_n_err 7.1840
---- Start Training Epoch 12: 401 batches ----
---- Start val Epoch 12: 33 batches ----
Save epoch model at 12
Epoch 12: train_loss 0.2760, val_n_err 5.6529
---- Start Training Epoch 13: 401 batches ----
---- Start val Epoch 13: 33 batches ----
Best model saved at epoch 13
Save epoch model at 13
Epoch 13: train_loss 0.2770, val_n_err 5.5621
---- Start Training Epoch 14: 401 batches ----
---- Start val Epoch 14: 33 batches ----
Save epoch model at 14
Epoch 14: train_loss 0.2768, val_n_err 5.8074
---- Start Training Epoch 15: 401 batches ----
---- Start val Epoch 15: 33 batches ----
Best model saved at epoch 15
Save epoch model at 15
Epoch 15: train_loss 0.2751, val_n_err 5.4833
---- Start Training Epoch 16: 401 batches ----
---- Start val Epoch 16: 33 batches ----
Save epoch model at 16
Epoch 16: train_loss 0.2719, val_n_err 5.6061
---- Start Training Epoch 17: 401 batches ----
---- Start val Epoch 17: 33 batches ----
Best model saved at epoch 17
Save epoch model at 17
Epoch 17: train_loss 0.2740, val_n_err 5.3394
---- Start Training Epoch 18: 401 batches ----
---- Start val Epoch 18: 33 batches ----
Best model saved at epoch 18
Save epoch model at 18
Epoch 18: train_loss 0.2758, val_n_err 5.2456
---- Start Training Epoch 19: 401 batches ----
---- Start val Epoch 19: 33 batches ----
Best model saved at epoch 19
Save epoch model at 19
Epoch 19: train_loss 0.2752, val_n_err 5.1597
---- Start Training Epoch 20: 401 batches ----
---- Start val Epoch 20: 33 batches ----
Save epoch model at 20
Epoch 20: train_loss 0.2734, val_n_err 5.2420
---- Start Training Epoch 21: 401 batches ----
---- Start val Epoch 21: 33 batches ----
Save epoch model at 21
Epoch 21: train_loss 0.2732, val_n_err 5.1945
---- Start Training Epoch 22: 401 batches ----
---- Start val Epoch 22: 33 batches ----
Best model saved at epoch 22
Save epoch model at 22
Epoch 22: train_loss 0.2750, val_n_err 5.0077
---- Start Training Epoch 23: 401 batches ----
---- Start val Epoch 23: 33 batches ----
Save epoch model at 23
Epoch 23: train_loss 0.2726, val_n_err 5.4569
---- Start Training Epoch 24: 401 batches ----
---- Start val Epoch 24: 33 batches ----
Best model saved at epoch 24
Save epoch model at 24
Epoch 24: train_loss 0.2726, val_n_err 4.9928
---- Start Training Epoch 25: 401 batches ----
---- Start val Epoch 25: 33 batches ----
Save epoch model at 25
Epoch 25: train_loss 0.2740, val_n_err 5.3493
---- Start Training Epoch 26: 401 batches ----
---- Start val Epoch 26: 33 batches ----
Save epoch model at 26
Epoch 26: train_loss 0.2736, val_n_err 5.1264
---- Start Training Epoch 27: 401 batches ----
---- Start val Epoch 27: 33 batches ----
Save epoch model at 27
Epoch 27: train_loss 0.2723, val_n_err 5.1561
---- Start Training Epoch 28: 401 batches ----
