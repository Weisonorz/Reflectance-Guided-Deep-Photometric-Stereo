{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "Conv pad = 1\n",
      "extractor.conv1.0.weight\n",
      "extractor.conv1.0.bias\n",
      "extractor.conv2.0.weight\n",
      "extractor.conv2.0.bias\n",
      "extractor.conv3.0.weight\n",
      "extractor.conv3.0.bias\n",
      "extractor.conv4.0.weight\n",
      "extractor.conv4.0.bias\n",
      "extractor.conv5.0.weight\n",
      "extractor.conv5.0.bias\n",
      "extractor.conv6.0.weight\n",
      "extractor.conv7.0.weight\n",
      "extractor.conv7.0.bias\n",
      "regressor.deconv1.0.weight\n",
      "regressor.deconv1.0.bias\n",
      "regressor.deconv2.0.weight\n",
      "regressor.deconv2.0.bias\n",
      "regressor.deconv3.0.weight\n",
      "regressor.est_normal.0.weight\n",
      "------------------------\n",
      "brdf_embed_block.embed_transform.weight\n",
      "brdf_embed_block.embed_transform.bias\n",
      "extractor.conv1.regular_conv.weight\n",
      "extractor.conv1.regular_conv.bias\n",
      "extractor.conv1.cBatchNorm.betas\n",
      "extractor.conv1.cBatchNorm.gammas\n",
      "extractor.conv1.cBatchNorm.running_mean\n",
      "extractor.conv1.cBatchNorm.running_var\n",
      "extractor.conv1.cBatchNorm.num_batches_tracked\n",
      "extractor.conv1.cBatchNorm.fc_gamma.0.weight\n",
      "extractor.conv1.cBatchNorm.fc_gamma.0.bias\n",
      "extractor.conv1.cBatchNorm.fc_gamma.2.weight\n",
      "extractor.conv1.cBatchNorm.fc_gamma.2.bias\n",
      "extractor.conv1.cBatchNorm.fc_beta.0.weight\n",
      "extractor.conv1.cBatchNorm.fc_beta.0.bias\n",
      "extractor.conv1.cBatchNorm.fc_beta.2.weight\n",
      "extractor.conv1.cBatchNorm.fc_beta.2.bias\n",
      "extractor.conv2.regular_conv.weight\n",
      "extractor.conv2.regular_conv.bias\n",
      "extractor.conv2.cBatchNorm.betas\n",
      "extractor.conv2.cBatchNorm.gammas\n",
      "extractor.conv2.cBatchNorm.running_mean\n",
      "extractor.conv2.cBatchNorm.running_var\n",
      "extractor.conv2.cBatchNorm.num_batches_tracked\n",
      "extractor.conv2.cBatchNorm.fc_gamma.0.weight\n",
      "extractor.conv2.cBatchNorm.fc_gamma.0.bias\n",
      "extractor.conv2.cBatchNorm.fc_gamma.2.weight\n",
      "extractor.conv2.cBatchNorm.fc_gamma.2.bias\n",
      "extractor.conv2.cBatchNorm.fc_beta.0.weight\n",
      "extractor.conv2.cBatchNorm.fc_beta.0.bias\n",
      "extractor.conv2.cBatchNorm.fc_beta.2.weight\n",
      "extractor.conv2.cBatchNorm.fc_beta.2.bias\n",
      "extractor.conv3.regular_conv.weight\n",
      "extractor.conv3.regular_conv.bias\n",
      "extractor.conv3.cBatchNorm.betas\n",
      "extractor.conv3.cBatchNorm.gammas\n",
      "extractor.conv3.cBatchNorm.running_mean\n",
      "extractor.conv3.cBatchNorm.running_var\n",
      "extractor.conv3.cBatchNorm.num_batches_tracked\n",
      "extractor.conv3.cBatchNorm.fc_gamma.0.weight\n",
      "extractor.conv3.cBatchNorm.fc_gamma.0.bias\n",
      "extractor.conv3.cBatchNorm.fc_gamma.2.weight\n",
      "extractor.conv3.cBatchNorm.fc_gamma.2.bias\n",
      "extractor.conv3.cBatchNorm.fc_beta.0.weight\n",
      "extractor.conv3.cBatchNorm.fc_beta.0.bias\n",
      "extractor.conv3.cBatchNorm.fc_beta.2.weight\n",
      "extractor.conv3.cBatchNorm.fc_beta.2.bias\n",
      "extractor.conv4.regular_conv.weight\n",
      "extractor.conv4.regular_conv.bias\n",
      "extractor.conv4.cBatchNorm.betas\n",
      "extractor.conv4.cBatchNorm.gammas\n",
      "extractor.conv4.cBatchNorm.running_mean\n",
      "extractor.conv4.cBatchNorm.running_var\n",
      "extractor.conv4.cBatchNorm.num_batches_tracked\n",
      "extractor.conv4.cBatchNorm.fc_gamma.0.weight\n",
      "extractor.conv4.cBatchNorm.fc_gamma.0.bias\n",
      "extractor.conv4.cBatchNorm.fc_gamma.2.weight\n",
      "extractor.conv4.cBatchNorm.fc_gamma.2.bias\n",
      "extractor.conv4.cBatchNorm.fc_beta.0.weight\n",
      "extractor.conv4.cBatchNorm.fc_beta.0.bias\n",
      "extractor.conv4.cBatchNorm.fc_beta.2.weight\n",
      "extractor.conv4.cBatchNorm.fc_beta.2.bias\n",
      "extractor.conv5.regular_conv.weight\n",
      "extractor.conv5.regular_conv.bias\n",
      "extractor.conv5.cBatchNorm.betas\n",
      "extractor.conv5.cBatchNorm.gammas\n",
      "extractor.conv5.cBatchNorm.running_mean\n",
      "extractor.conv5.cBatchNorm.running_var\n",
      "extractor.conv5.cBatchNorm.num_batches_tracked\n",
      "extractor.conv5.cBatchNorm.fc_gamma.0.weight\n",
      "extractor.conv5.cBatchNorm.fc_gamma.0.bias\n",
      "extractor.conv5.cBatchNorm.fc_gamma.2.weight\n",
      "extractor.conv5.cBatchNorm.fc_gamma.2.bias\n",
      "extractor.conv5.cBatchNorm.fc_beta.0.weight\n",
      "extractor.conv5.cBatchNorm.fc_beta.0.bias\n",
      "extractor.conv5.cBatchNorm.fc_beta.2.weight\n",
      "extractor.conv5.cBatchNorm.fc_beta.2.bias\n",
      "extractor.conv6.0.weight\n",
      "extractor.conv7.0.weight\n",
      "extractor.conv7.0.bias\n",
      "regressor.deconv1.0.weight\n",
      "regressor.deconv1.0.bias\n",
      "regressor.deconv2.0.weight\n",
      "regressor.deconv2.0.bias\n",
      "regressor.deconv3.0.weight\n",
      "regressor.est_normal.0.weight\n"
     ]
    }
   ],
   "source": [
    "from models import PS_FCN_CBN, PS_FCN \n",
    "from datasets import PS_Blobby_BRDF_Dataset \n",
    "import torch \n",
    "import re\n",
    "\n",
    "model_cbn = PS_FCN_CBN(c_in=6) \n",
    "model = PS_FCN(c_in=6) \n",
    "# checkpoint = torch.load('/data2/datasets/ruoguli/idl_project_datas/PS-FCN_B_S_32.pth.tar')\n",
    "# checkpoint['state_dict']\n",
    "for key in model.state_dict().keys():\n",
    "\tprint(key)\n",
    "print('------------------------')\n",
    "for key in model_cbn.state_dict().keys():\n",
    "\tprint(key)\n",
    "# model.load_state_dict(checkpoint) \n",
    "\t\n",
    "\n",
    "def loadCheckpoint_to_PSFCN_CBN(path, model, cuda=True):\n",
    "    \"\"\"\n",
    "    Loads weights from a pretrained PS_FCN checkpoint into a PS_FCN_CBN model.\n",
    "\n",
    "    Handles the mapping between standard Conv layers in PS_FCN's extractor\n",
    "    and the Conditional_Conv layers (specifically the 'regular_conv' part)\n",
    "    in PS_FCN_CBN's extractor. Assumes batchNorm=False was used for the\n",
    "    original model's convolutional layers that are being mapped.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    print(f\"Loading pretrained weights for PS_FCN_CBN from: {path}\")\n",
    "    pretrained_state_dict = checkpoint['state_dict']\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # 1. Create a new state dict to hold the weights to load\n",
    "    new_state_dict = {}\n",
    "    loaded_keys = set()\n",
    "    skipped_keys_mismatch = set()\n",
    "    skipped_keys_nonexistent = set()\n",
    "    original_keys_mapped = set()\n",
    "\n",
    "    # 2. Iterate through the pretrained state dict\n",
    "    for k_pretrained, v_pretrained in pretrained_state_dict.items():\n",
    "        original_keys_mapped.add(k_pretrained) # Track keys from original model\n",
    "\n",
    "        # Try direct mapping first (covers Regressor and extractor.conv6, extractor.conv7)\n",
    "        if k_pretrained in model_dict and model_dict[k_pretrained].shape == v_pretrained.shape:\n",
    "            new_state_dict[k_pretrained] = v_pretrained\n",
    "            loaded_keys.add(k_pretrained)\n",
    "            # print(f\"Directly loaded: {k_pretrained}\")\n",
    "            continue # Go to next key\n",
    "\n",
    "        # --- Special Mapping for extractor.conv1 to conv5 ---\n",
    "        # Check if it's a weight from the original extractor's conv layers 1-5\n",
    "        match = re.match(r'extractor\\.conv(\\d+)\\.0\\.weight', k_pretrained)\n",
    "        if match:\n",
    "            layer_num = match.group(1)\n",
    "            # Construct the corresponding key in the new model\n",
    "            k_new = f'extractor.conv{layer_num}.regular_conv.weight'\n",
    "\n",
    "            if k_new in model_dict:\n",
    "                if model_dict[k_new].shape == v_pretrained.shape:\n",
    "                    new_state_dict[k_new] = v_pretrained\n",
    "                    loaded_keys.add(k_new)\n",
    "                    # print(f\"Mapped: {k_pretrained} -> {k_new}\")\n",
    "                else:\n",
    "                    # print(f\"Shape mismatch: {k_pretrained} ({v_pretrained.shape}) vs {k_new} ({model_dict[k_new].shape})\")\n",
    "                    skipped_keys_mismatch.add(k_pretrained)\n",
    "            else:\n",
    "                # print(f\"Key not found in new model: {k_new} (mapped from {k_pretrained})\")\n",
    "                skipped_keys_nonexistent.add(k_pretrained)\n",
    "            continue # Go to next key\n",
    "\n",
    "        # Check if it's a bias from the original extractor's conv layers 1-5\n",
    "        # These biases don't exist in the new model's 'regular_conv' (bias=False)\n",
    "        match_bias = re.match(r'extractor\\.conv(\\d+)\\.0\\.bias', k_pretrained)\n",
    "        if match_bias and int(match_bias.group(1)) <= 5 :\n",
    "            # print(f\"Skipped (no bias in new layer): {k_pretrained}\")\n",
    "            skipped_keys_nonexistent.add(k_pretrained) # Bias intentionally skipped\n",
    "            continue # Go to next key\n",
    "\n",
    "        # If we reach here, the key was not directly matched or specifically mapped/skipped\n",
    "        # print(f\"Skipped (no match/mapping rule): {k_pretrained}\")\n",
    "        if k_pretrained in model_dict:\n",
    "             skipped_keys_mismatch.add(k_pretrained) # Likely shape mismatch if key exists but wasn't caught above\n",
    "        else:\n",
    "             skipped_keys_nonexistent.add(k_pretrained) # Key doesn't exist in target\n",
    "\n",
    "\n",
    "    # 3. Update the model's state dict with the mapped weights\n",
    "    model_dict.update(new_state_dict)\n",
    "\n",
    "    # 4. Load the updated state dict into the model\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    # 5. Report Summary\n",
    "    print(\"-\" * 50)\n",
    "    print(\"State Dict Loading Summary:\")\n",
    "    print(f\"  Loaded {len(loaded_keys)} weights.\")\n",
    "    # Optionally print details\n",
    "    # print(\"  Loaded Keys:\", sorted(list(loaded_keys)))\n",
    "\n",
    "    print(f\"  Skipped {len(skipped_keys_mismatch)} pretrained keys due to shape mismatch.\")\n",
    "    # print(\"  Skipped Mismatch Keys:\", sorted(list(skipped_keys_mismatch)))\n",
    "\n",
    "    print(f\"  Skipped {len(skipped_keys_nonexistent)} pretrained keys (no target layer/param or intentionally ignored like old biases).\")\n",
    "    # print(\"  Skipped Non-existent/Ignored Keys:\", sorted(list(skipped_keys_nonexistent)))\n",
    "\n",
    "    # Check for keys in the new model that were *not* loaded\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    not_loaded = model_keys - loaded_keys\n",
    "    print(f\"  {len(not_loaded)} keys in the new model were not loaded from checkpoint (initialized instead).\")\n",
    "    # print(\"  Not Loaded Keys:\", sorted(list(not_loaded))) # This will include CBN params, BRDF embedder, etc.\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1029293/591810211.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStandard loading complete. Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloaded_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parameters, skipped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskipped_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from pretrained dict.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(model.state_dict())\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(new_state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parameters in the new model were initialized.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[43mloadCheckpoint_to_PSFCN_CBN_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/data2/datasets/ruoguli/idl_project_datas/PS-FCN_B_S_32.pth.tar\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_cbn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mloadCheckpoint_to_PSFCN_CBN_debug\u001b[39m\u001b[34m(path, model, cuda, output_log_file)\u001b[39m\n\u001b[32m     26\u001b[39m log_lines.append(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cuda:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     31\u001b[39m     checkpoint = torch.load(path, map_location=\u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:1384\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1383\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:1638\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n\u001b[32m   1636\u001b[39m unpickler = UnpicklerWrapper(f, **pickle_load_args)\n\u001b[32m   1637\u001b[39m unpickler.persistent_load = persistent_load\n\u001b[32m-> \u001b[39m\u001b[32m1638\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1640\u001b[39m deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n\u001b[32m   1642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._guards.active_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:1566\u001b[39m, in \u001b[36m_legacy_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1564\u001b[39m     obj = cast(Storage, torch.UntypedStorage(nbytes))\n\u001b[32m   1565\u001b[39m     obj._torch_load_uninitialized = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1566\u001b[39m     obj = \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   1568\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   1569\u001b[39m typed_storage = torch.storage.TypedStorage(\n\u001b[32m   1570\u001b[39m     wrap_storage=obj, dtype=dtype, _internal=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:601\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    583\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    603\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:539\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    537\u001b[39m     backend_name = torch._C._get_privateuse1_backend_name()\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     device = \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.to(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:508\u001b[39m, in \u001b[36m_validate_device\u001b[39m\u001b[34m(location, backend_name)\u001b[39m\n\u001b[32m    506\u001b[39m     device_index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mis_available\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.is_available() is False. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you are running on a CPU-only machine, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto map your storages to the CPU.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mdevice_count\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    516\u001b[39m     device_count = device_module.device_count()\n",
      "\u001b[31mRuntimeError\u001b[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time # For unique log file names\n",
    "\n",
    "# --- Keep your existing model_utils functions here ---\n",
    "# getInput, parseData, getInputChanel, get_n_params, loadCheckpoint_to_PSFCN,\n",
    "# saveCheckpoint, conv, deconv\n",
    "\n",
    "\n",
    "# --- Add this UPDATED DETAILED logging version ---\n",
    "def loadCheckpoint_to_PSFCN_CBN_debug(path, model, cuda=True, output_log_file=None):\n",
    "    \"\"\"\n",
    "    Loads weights from a pretrained PS_FCN checkpoint into a PS_FCN_CBN model.\n",
    "    Logs detailed information about loaded, skipped, and uninitialized weights\n",
    "    to the specified output_log_file.\n",
    "\n",
    "    Handles mapping for weights AND biases for extractor.conv1-5.\n",
    "    Assumes original model used model_utils.conv(batchNorm=False).\n",
    "    \"\"\"\n",
    "    if output_log_file is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        output_log_file = f\"loading_log_{timestamp}.txt\"\n",
    "\n",
    "    log_lines = []\n",
    "    log_lines.append(f\"--- Loading Checkpoint: {path} ---\")\n",
    "    log_lines.append(f\"--- Target Model: {type(model).__name__} ---\")\n",
    "    log_lines.append(\"-\" * 50)\n",
    "\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    log_lines.append(f\"Checkpoint loaded successfully from: {path}\")\n",
    "    pretrained_state_dict = checkpoint.get('state_dict', checkpoint) # Handle cases where checkpoint might just be the state_dict\n",
    "    model_dict = model.state_dict()\n",
    "    new_model_keys = set(model_dict.keys())\n",
    "\n",
    "    new_state_dict = {}\n",
    "    loaded_keys_list = []\n",
    "    skipped_mismatch_list = []\n",
    "    skipped_nonexistent_list = []\n",
    "    # removed skipped_bias_list as biases are now loaded\n",
    "    original_keys_processed = set()\n",
    "\n",
    "    log_lines.append(\"\\n--- Processing Pretrained Parameters ---\")\n",
    "    for k_pretrained, v_pretrained in pretrained_state_dict.items():\n",
    "        original_keys_processed.add(k_pretrained)\n",
    "        processed = False\n",
    "\n",
    "        # --- Special Mapping for extractor.conv1 to conv5 Weights ---\n",
    "        match_weight = re.match(r'extractor\\.conv(\\d+)\\.0\\.weight', k_pretrained)\n",
    "        if match_weight and int(match_weight.group(1)) <= 5:\n",
    "            layer_num = match_weight.group(1)\n",
    "            k_new = f'extractor.conv{layer_num}.regular_conv.weight'\n",
    "            if k_new in model_dict:\n",
    "                if model_dict[k_new].shape == v_pretrained.shape:\n",
    "                    new_state_dict[k_new] = v_pretrained\n",
    "                    loaded_keys_list.append(f\"{k_pretrained} -> {k_new} (Shape: {tuple(v_pretrained.shape)})\")\n",
    "                    processed = True\n",
    "                else:\n",
    "                    skipped_mismatch_list.append(f\"{k_pretrained} (Shape: {tuple(v_pretrained.shape)}) vs {k_new} (Shape: {tuple(model_dict[k_new].shape)})\")\n",
    "                    processed = True\n",
    "            else:\n",
    "                 skipped_nonexistent_list.append(f\"{k_pretrained} (Target key {k_new} not found in new model)\")\n",
    "                 processed = True\n",
    "            # Continue to next key in outer loop IF processed, otherwise might be handled by bias rule or direct match\n",
    "            if processed: continue\n",
    "\n",
    "        # --- Special Mapping for extractor.conv1 to conv5 Biases ---\n",
    "        match_bias = re.match(r'extractor\\.conv(\\d+)\\.0\\.bias', k_pretrained)\n",
    "        if match_bias and int(match_bias.group(1)) <= 5:\n",
    "            layer_num = match_bias.group(1)\n",
    "            k_new = f'extractor.conv{layer_num}.regular_conv.bias' # Target is now the bias of regular_conv\n",
    "            if k_new in model_dict:\n",
    "                if model_dict[k_new].shape == v_pretrained.shape:\n",
    "                    new_state_dict[k_new] = v_pretrained\n",
    "                    loaded_keys_list.append(f\"{k_pretrained} -> {k_new} (Shape: {tuple(v_pretrained.shape)})\")\n",
    "                    processed = True\n",
    "                else:\n",
    "                    skipped_mismatch_list.append(f\"{k_pretrained} (Shape: {tuple(v_pretrained.shape)}) vs {k_new} (Shape: {tuple(model_dict[k_new].shape)})\")\n",
    "                    processed = True\n",
    "            else:\n",
    "                 skipped_nonexistent_list.append(f\"{k_pretrained} (Target key {k_new} not found in new model)\")\n",
    "                 processed = True\n",
    "            # Continue to next key in outer loop IF processed\n",
    "            if processed: continue\n",
    "\n",
    "        # --- Try Direct Mapping for other keys (Regressor, extractor.conv6/7 etc.) ---\n",
    "        # This rule will now handle keys like extractor.conv7.0.bias directly too.\n",
    "        if not processed:\n",
    "            if k_pretrained in model_dict:\n",
    "                if model_dict[k_pretrained].shape == v_pretrained.shape:\n",
    "                    new_state_dict[k_pretrained] = v_pretrained\n",
    "                    loaded_keys_list.append(f\"{k_pretrained} -> {k_pretrained} (Direct Match, Shape: {tuple(v_pretrained.shape)})\")\n",
    "                    processed = True\n",
    "                else:\n",
    "                    skipped_mismatch_list.append(f\"{k_pretrained} (Shape: {tuple(v_pretrained.shape)}) vs {k_pretrained} (Shape: {tuple(model_dict[k_pretrained].shape)})\")\n",
    "                    processed = True\n",
    "                # Continue to next key in outer loop IF processed\n",
    "                if processed: continue\n",
    "\n",
    "        # --- If not processed by any rule above, it's skipped as non-existent in target ---\n",
    "        if not processed:\n",
    "             skipped_nonexistent_list.append(f\"{k_pretrained} (No matching key or rule in new model)\")\n",
    "\n",
    "\n",
    "    log_lines.append(\"\\n--- Loading into Model ---\")\n",
    "    model_dict.update(new_state_dict)\n",
    "    model.load_state_dict(model_dict, strict=True) # strict=True ensures all model_dict keys are consumed\n",
    "    log_lines.append(\"model.load_state_dict(model_dict) executed.\")\n",
    "\n",
    "\n",
    "    # --- Report Summary ---\n",
    "    log_lines.append(\"\\n\" + \"=\" * 50)\n",
    "    log_lines.append(\"      State Dict Loading Summary\")\n",
    "    log_lines.append(\"=\" * 50)\n",
    "\n",
    "    # Loaded Keys\n",
    "    log_lines.append(f\"\\n--- ({len(loaded_keys_list)}) Keys Loaded Successfully ---\")\n",
    "    loaded_keys_list.sort()\n",
    "    log_lines.extend(loaded_keys_list)\n",
    "\n",
    "    # Skipped Keys (Shape Mismatch)\n",
    "    log_lines.append(f\"\\n--- ({len(skipped_mismatch_list)}) Pretrained Keys Skipped (Shape Mismatch) ---\")\n",
    "    skipped_mismatch_list.sort()\n",
    "    log_lines.extend(skipped_mismatch_list)\n",
    "\n",
    "    # Skipped Keys (Non-Existent Target)\n",
    "    log_lines.append(f\"\\n--- ({len(skipped_nonexistent_list)}) Pretrained Keys Skipped (No Target Layer/Param) ---\")\n",
    "    skipped_nonexistent_list.sort()\n",
    "    log_lines.extend(skipped_nonexistent_list)\n",
    "\n",
    "    # Keys in New Model NOT Loaded\n",
    "    final_loaded_keys = set(new_state_dict.keys())\n",
    "    not_loaded_new_keys = new_model_keys - final_loaded_keys\n",
    "    not_loaded_list = sorted(list(not_loaded_new_keys))\n",
    "    log_lines.append(f\"\\n--- ({len(not_loaded_list)}) Keys in New Model NOT Loaded from Checkpoint (Initialized Instead) ---\")\n",
    "    log_lines.extend(not_loaded_list)\n",
    "    log_lines.append(\"=\" * 50)\n",
    "\n",
    "\n",
    "    # --- Write log to file ---\n",
    "    try:\n",
    "        with open(output_log_file, 'w') as f:\n",
    "            for line in log_lines:\n",
    "                f.write(line + '\\n')\n",
    "        print(f\"Detailed loading log saved to: {output_log_file}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing log file {output_log_file}: {e}\")\n",
    "        print(\"\\n--- Log Output (Console) ---\")\n",
    "        for line in log_lines:\n",
    "            print(line)\n",
    "\n",
    "# --- Also update the standard (non-debug) loader for consistency ---\n",
    "def loadCheckpoint_to_PSFCN_CBN(path, model, cuda=True):\n",
    "    \"\"\"\n",
    "    Loads weights from a pretrained PS_FCN checkpoint into a PS_FCN_CBN model.\n",
    "    Handles mapping for weights AND biases for extractor.conv1-5.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    print(f\"Loading pretrained weights for PS_FCN_CBN from: {path} (using standard loader)\")\n",
    "    pretrained_state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "    model_dict = model.state_dict()\n",
    "    new_state_dict = {}\n",
    "    loaded_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for k_pretrained, v_pretrained in pretrained_state_dict.items():\n",
    "        mapped = False\n",
    "        # Special Mapping for extractor.conv1 to conv5 Weights\n",
    "        match_weight = re.match(r'extractor\\.conv(\\d+)\\.0\\.weight', k_pretrained)\n",
    "        if match_weight and int(match_weight.group(1)) <= 5:\n",
    "            layer_num = match_weight.group(1)\n",
    "            k_new = f'extractor.conv{layer_num}.regular_conv.weight'\n",
    "            if k_new in model_dict and model_dict[k_new].shape == v_pretrained.shape:\n",
    "                new_state_dict[k_new] = v_pretrained\n",
    "                mapped = True\n",
    "            if mapped: continue\n",
    "\n",
    "        # Special Mapping for extractor.conv1 to conv5 Biases\n",
    "        match_bias = re.match(r'extractor\\.conv(\\d+)\\.0\\.bias', k_pretrained)\n",
    "        if match_bias and int(match_bias.group(1)) <= 5:\n",
    "            layer_num = match_bias.group(1)\n",
    "            k_new = f'extractor.conv{layer_num}.regular_conv.bias'\n",
    "            if k_new in model_dict and model_dict[k_new].shape == v_pretrained.shape:\n",
    "                new_state_dict[k_new] = v_pretrained\n",
    "                mapped = True\n",
    "            if mapped: continue\n",
    "\n",
    "        # Direct mapping for others\n",
    "        if not mapped:\n",
    "            if k_pretrained in model_dict and model_dict[k_pretrained].shape == v_pretrained.shape:\n",
    "                new_state_dict[k_pretrained] = v_pretrained\n",
    "                mapped = True\n",
    "\n",
    "        if mapped:\n",
    "            loaded_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "\n",
    "\n",
    "    model_dict.update(new_state_dict)\n",
    "    model.load_state_dict(model_dict, strict=True) # Use strict=True\n",
    "\n",
    "    print(f\"Standard loading complete. Loaded {loaded_count} parameters, skipped {skipped_count} from pretrained dict.\")\n",
    "    print(f\"{len(model.state_dict()) - len(new_state_dict)} parameters in the new model were initialized.\")\n",
    "loadCheckpoint_to_PSFCN_CBN_debug('/data2/datasets/ruoguli/idl_project_datas/PS-FCN_B_S_32.pth.tar', model_cbn, cuda=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
