=> Using adam solver for optimization
=> Using cos for criterion normal
/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
---- Start Training Epoch 1: 802 batches ----
Train:   0%|                                         | 0/802 [00:00<?, ?it/s]/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Traceback (most recent call last):
  File "/home/ruoguli/idl_project/train_PSFCN.py", line 56, in <module>
    main(args)
  File "/home/ruoguli/idl_project/train_PSFCN.py", line 39, in main
    train_loss = train_utils.train(args, train_loader, model, criterion, optimizer, epoch)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruoguli/idl_project/train_utils.py", line 25, in train
    criterion.backward(); timer.updateTime('Backward')
    ^^^^^^^^^^^^^^^^^^^^
  File "/home/ruoguli/idl_project/models/solver_utils.py", line 36, in backward
    self.loss.backward()
  File "/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/data2/datasets/ruoguli/miniconda/envs/torch_env/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1788, in backward
    assert (
AssertionError:     We incorrectly attempted to compile the backward with incorrect subclass metadata.
    If you run into this error, please file an issue.
    Expected grad_output types: [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]
    Got grad_output types: [<class 'torch.Tensor'>, <class 'NoneType'>, <class 'torch.Tensor'>]
